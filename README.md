# ğŸš€ ModelQuants

[![Python Version](https://img.shields.io/badge/python-3.8%2B-blue.svg)](https://python.org)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Platform](https://img.shields.io/badge/platform-Windows%20%7C%20Linux%20%7C%20macOS-lightgrey.svg)](https://github.com)
[![HuggingFace](https://img.shields.io/badge/ğŸ¤—-HuggingFace-yellow.svg)](https://huggingface.co)

**Professional Model Quantization Converter for HuggingFace Transformers**

ModelQuants is a state-of-the-art GUI application designed for AI researchers, engineers, and enthusiasts who need to efficiently quantize large language models. Convert your BF16/FP16 models to optimized 4-bit or 8-bit formats with a single click, dramatically reducing memory usage while maintaining model performance.

![ModelQuants Screenshot](https://github.com/LMLK-seal/ModelQuants/blob/main/screenshot.png?raw=true)

---

## âœ¨ Features

### ğŸ¯ **Core Functionality**
- **ğŸ”§ Advanced Quantization**: Support for 4-bit (NF4/FP4) and 8-bit quantization using BitsAndBytesConfig
- **ğŸ“Š Real-time Progress**: Live progress tracking with detailed status updates
- **ğŸ›¡ï¸ Model Validation**: Comprehensive model structure validation before processing
- **ğŸ’¾ Memory Optimization**: Automatic memory cleanup and CUDA cache management
- **ğŸ” Debug Tools**: Built-in diagnostic tools for troubleshooting model paths

### ğŸ–¥ï¸ **Professional Interface**
- **ğŸ¨ Modern Dark Theme**: Sleek customtkinter-based GUI with professional aesthetics
- **ğŸ“ Smart Path Management**: Auto-suggestion of output paths and intelligent folder selection
- **ğŸ“ˆ Model Information Display**: Automatic detection and display of model architecture details
- **âš¡ Threaded Processing**: Non-blocking UI with background quantization processing
- **ğŸš¨ Error Handling**: Robust error management with user-friendly notifications

### ğŸ”§ **Technical Excellence**
- **ğŸ“ Comprehensive Logging**: Detailed logging to both file and console for debugging
- **ğŸ”’ Thread Safety**: Safe multi-threaded operations with proper synchronization
- **ğŸ’¡ Intelligent Validation**: Deep model structure analysis and file integrity checks
- **ğŸ¯ Precision Control**: Fine-tuned quantization parameters for optimal results

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.8+** ğŸ
- **CUDA-compatible GPU** (recommended) âš¡
- **8GB+ System RAM** (16GB+ recommended for large models) ğŸ’¾

### ğŸ“¦ Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/LMLK-seal/ModelQuants.git
   cd ModelQuants
   ```

2. install manually:
   ```bash
   pip install torch transformers accelerate bitsandbytes customtkinter
   ```

3. **Run ModelQuants:**
   ```bash
   python ModelQuants.py
   ```

---

## ğŸ“– Usage Guide

### ğŸ¯ **Basic Workflow**

1. **ğŸ“‚ Select Model**: Choose your HuggingFace model folder
2. **ğŸ“ Set Output**: Specify where to save the quantized model
3. **âš™ï¸ Choose Quantization**: Select your preferred quantization type
4. **ğŸš€ Start Process**: Click "Start Quantization" and monitor progress

### ğŸ”§ **Quantization Options**

| Type | Description | Use Case | Memory Reduction |
|------|-------------|----------|------------------|
| **4-bit (NF4)** | ğŸ¥‡ Normalized Float 4-bit | **Recommended** - Best quality/size ratio | ~75% |
| **4-bit (FP4)** | ğŸ¥ˆ Float 4-bit | Alternative 4-bit option | ~75% |
| **8-bit** | ğŸ¥‰ Integer 8-bit | Conservative quantization | ~50% |

### ğŸ“Š **Model Information Display**

ModelQuants automatically detects and displays:
- ğŸ—ï¸ **Model Architecture**: Type and structure information
- ğŸ”¢ **Parameter Count**: Total model parameters (B/M format)
- ğŸ“ **Hidden Size**: Model dimension specifications
- ğŸ§  **Layer Count**: Number of transformer layers
- ğŸ“š **Vocabulary Size**: Tokenizer vocabulary information

---

## ğŸ› ï¸ Advanced Features

### ğŸ” **Debug Tools**

Use the **Debug Path** button to diagnose model loading issues:
- ğŸ“ Directory structure analysis
- ğŸ”— Symlink resolution
- ğŸ“„ File listing and validation
- âš™ï¸ Configuration file inspection

### ğŸ“ **Logging System**

ModelQuants maintains comprehensive logs:
- ğŸ“Š **Console Output**: Real-time processing information
- ğŸ“ **File Logging**: Persistent logs saved to `quantizer.log`
- ğŸš¨ **Error Tracking**: Detailed error traces for debugging

### ğŸ›ï¸ **Configuration**

Advanced users can modify quantization parameters in the source code:
```python
QUANTIZATION_CONFIGS = {
    "4-bit (NF4)": {
        "load_in_4bit": True,
        "bnb_4bit_quant_type": "nf4",
        "bnb_4bit_use_double_quant": True,
        "bnb_4bit_compute_dtype": torch.bfloat16
    }
}
```

---

## ğŸ“‹ Requirements

### ğŸ–¥ï¸ **System Requirements**

| Component | Minimum | Recommended |
|-----------|---------|-------------|
| **OS** | Windows 10/Linux/macOS | Windows 11/Ubuntu 20.04+ |
| **RAM** | 8GB | 16GB+ |
| **GPU** | CUDA 11.0+ | RTX 3080/4080+ |
| **Storage** | 50GB free | 100GB+ SSD |

### ğŸ“¦ **Python Dependencies**

```
torch>=2.0.0
transformers>=4.30.0
accelerate>=0.20.0
bitsandbytes>=0.39.0
customtkinter>=5.0.0
```

---

## ğŸ”§ Troubleshooting

### â“ **Common Issues**

**ğŸš¨ "Invalid model folder" error:**
- Use the Debug Path button to analyze the directory
- Ensure `config.json` and model weight files are present
- Check for symlink or permission issues

**ğŸ’¾ "Out of memory" error:**
- Close other GPU-intensive applications
- Try 8-bit quantization first for large models
- Ensure sufficient system RAM

**âš¡ "CUDA not available" warning:**
- Install CUDA-compatible PyTorch version
- Verify GPU drivers are up to date
- CPU quantization is supported but slower

### ğŸ“ **Getting Help**

1. ğŸ” Check the debug output using the Debug Path button
2. ğŸ“ Review the `quantizer.log` file for detailed error information
3. ğŸ› Open an issue with system specs and error logs
4. ğŸ’¬ Join our community discussions

---

## ğŸ¤ Contributing

We welcome contributions! Here's how you can help:

### ğŸ¯ **Ways to Contribute**
- ğŸ› **Bug Reports**: Submit detailed issue reports
- ğŸ’¡ **Feature Requests**: Suggest new functionality
- ğŸ”§ **Code Contributions**: Submit pull requests
- ğŸ“š **Documentation**: Improve guides and examples

### ğŸ“ **Coding Standards**
- Follow PEP 8 style guidelines
- Include type hints for new functions
- Add comprehensive docstrings
- Write unit tests for new features

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

```
MIT License

Copyright (c) 2024 ModelQuants Contributors

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.
```

---

## ğŸŒŸ Acknowledgments

- ğŸ¤— **HuggingFace Team** for the transformers ecosystem
- ğŸ”§ **BitsAndBytesConfig** for quantization algorithms
- ğŸ¨ **CustomTkinter** for the modern GUI framework
- ğŸš€ **PyTorch Team** for the underlying ML framework
- ğŸ‘¥ **Open Source Community** for continuous inspiration

---

## ğŸ“Š Project Stats

![GitHub stars](https://img.shields.io/github/stars/LMLK-seal/modelquants?style=social)
![GitHub forks](https://img.shields.io/github/forks/LMLK-seal/modelquants?style=social)
![GitHub issues](https://img.shields.io/github/issues/LMLK-seal/modelquants)
![GitHub pull requests](https://img.shields.io/github/issues-pr/LMLK-seal/modelquants)

---

<div align="center">

**â­ Star this repository if ModelQuants helped you optimize your models! â­**

[ğŸ› Report Bug](https://github.com/LMLK-seal/modelquants/issues) â€¢ [ğŸ’¡ Request Feature](https://github.com/LMLK-seal/modelquants/issues) â€¢ [ğŸ’¬ Discussions](https://github.com/LMLK-seal/modelquants/discussions)

</div>
